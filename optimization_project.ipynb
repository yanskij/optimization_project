{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a problem. We don't have normal input for our problem and check something \"by hands\" is also complicated, because of problem is too hard to get an analytic solutuion. Sorry :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as sps\n",
    "import scipy.optimize as spo\n",
    "from cvxopt import solvers, matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2 # number of points minus 1 of start point\n",
    "A = np.diag(np.zeros(n) + 1) + np.diag(np.zeros(n - 1) + 1, k=-1)\n",
    "theta = np.pi / 2 # given theta_1, other theta_i comes our from theta_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = np.array([1, 1])\n",
    "b = np.array([theta, np.pi])\n",
    "x_start = np.linalg.inv(A) @ b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our real function to minimize and it's derivative. We drop the first coordinate because $\\psi_1$ doesn't defined. Derivative of F in zero we define as a zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F(x):\n",
    "    x = x[1:]\n",
    "    L = l[1:]\n",
    "    return np.array(L * x / np.sin(x))\n",
    "\n",
    "def dF(x):\n",
    "    x = x[1:]\n",
    "    L = l[1:]\n",
    "    return np.concatenate(([0], L * (np.sin(x) - x * np.cos(x)) / np.sin(x) ** 2))\n",
    "\n",
    "def hessian(x):\n",
    "    x = x[1:]\n",
    "    L = l[1:]\n",
    "    return np.diag(np.concatenate(([0], L * (1 + 2 * x * np.cos(x) ** 2) / np.sin(x) ** 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function from cvxopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acent(A, b):\n",
    "    return solvers.cp(F, A=A, b=b)['x']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x_start, eps=1e-4, alpha = 1e-2):\n",
    "    step_counter = 0\n",
    "    x_cur = x_start\n",
    "    x_new = x_cur - alpha * dF(x_cur)\n",
    "    while np.abs(np.linalg.norm(x_cur - x_new)) > eps and np.linalg.norm(dF(x_new)) > eps:\n",
    "        step_counter += 1\n",
    "        if step_counter > 10000:\n",
    "            raise Exception('For eps = %f and alpha = %f method diverges :('%(eps, alpha))\n",
    "        if np.min(x_cur) < -np.pi or np.max(x_cur) > np.pi:\n",
    "            raise Exception('For eps = %f and alpha = %f method comes out from the definition area:('%(eps, alpha))\n",
    "        x_cur = x_new\n",
    "        x_new = x_new = x_cur - alpha * dF(x_cur)\n",
    "    print('Optimal solution for eps = %f, alpha = %f founded'%( eps, alpha))\n",
    "    print('steps: %d' %(step_counter))\n",
    "    print('x_start =',(x_start))\n",
    "    print('x_optimal =',(x_cur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def newton_method(x_start, eps=1, alpha=1):\n",
    "    step_counter = 0\n",
    "    x_cur = x_start\n",
    "    x_new = x_cur - np.concatenate(([0], alpha * F(x_cur) / dF(x_cur)[1:]))\n",
    "    while np.abs(np.linalg.norm(x_cur - x_new)) > eps and np.linalg.norm(F(x_new)) > eps:\n",
    "        step_counter += 1\n",
    "        if step_counter > 10000:\n",
    "            raise Exception('For eps = %f and alpha = %f method diverges :('%(eps, alpha))\n",
    "        if np.min(x_cur) < -np.pi or np.max(x_cur) > np.pi:\n",
    "            raise Exception('For eps = %f and alpha = %f method comes out from the definition area:('%(eps, alpha))\n",
    "        x_cur = x_new\n",
    "        x_new = x_cur - np.concatenate(([0], alpha * F(x_cur)[1:] / dF(x_cur)[1:]))\n",
    "    print('Optimal solution for eps = %f, alpha = %f founded'%( eps, alpha))\n",
    "    print('steps: %d' %(step_counter))\n",
    "    print('x_start =',(x_start))\n",
    "    print('x_optimal =',(x_cur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal solution for eps = 0.000000, alpha = 0.100000 founded\n",
      "steps: 523\n",
      "x_start = [1.57079633 1.57079633]\n",
      "x_optimal = [1.57079633e+00 2.26893082e-08]\n",
      "Optimal solution for eps = 0.000000, alpha = 0.100000 founded\n",
      "steps: 1\n",
      "x_start = [1.57079633 1.57079633]\n",
      "x_optimal = [1.57079633 1.41371669]\n"
     ]
    }
   ],
   "source": [
    "gradient_descent(x_start, 1e-10, 1e-1)\n",
    "newton_method(x_start, 1e-10, 1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BFGS(F, dF, x_start, eps=10e-10):\n",
    "    \n",
    "    step_counter = 0\n",
    "    I = np.eye(len(x_start))\n",
    "    H = I\n",
    "    x_cur = x_start\n",
    "   \n",
    "    while np.log(np.linalg.norm(dF(x_cur))) > eps and step_counter < 10000:\n",
    "        \n",
    "        p = -H @ dF(x_cur)\n",
    "        \n",
    "        # Line search constants for the Wolfe conditions.\n",
    "        # Repeating the line search\n",
    "        \n",
    "        # line_search returns not only alpha\n",
    "        # but only this value is interesting for us\n",
    "\n",
    "        alpha = spo.optimize.line_search(F, dF, x_cur, p)[0] \n",
    "        \n",
    "        x_new = x_cur + alpha * p\n",
    "        s = x_new - x_cur\n",
    "        y = dF(x_new) - dF(x_cur)\n",
    "                           \n",
    "        x_cur = x_new\n",
    "        \n",
    "        step_counter += 1\n",
    "        \n",
    "        rho = 1.0 / (y.T @ s)\n",
    "        A1 = I - rho * s[:, np.newaxis] * y[np.newaxis, :]\n",
    "        A2 = I - rho * y[:, np.newaxis] * s[np.newaxis, :]\n",
    "        H = np.dot(A1, np.dot(H, A2)) + (rho * s[:, np.newaxis] *\n",
    "                                                 s[np.newaxis, :])\n",
    "    \n",
    "    \n",
    "    print('Result of BFGS method:')\n",
    "    print('Final Result (best point):', (x_cur))\n",
    "    print('Steps: %d' % (step_counter))\n",
    "    \n",
    "    #return (x_cur, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F(t):\n",
    "    x = t[0]\n",
    "    y = t[1]\n",
    "    return x ** 2 - x * y + y ** 2 + 9 * x - 6 * y + 20\n",
    "\n",
    "def dF(t):\n",
    "    x = t[0]\n",
    "    y = t[1]\n",
    "    return np.array([2 * x - y + 9, -x + 2 * y - 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of BFGS method:\n",
      "Final Result (best point): [-4.10204082  0.87244898]\n",
      "Steps: 2\n"
     ]
    }
   ],
   "source": [
    "x_start = (1, 1)\n",
    "eps = 1e-10\n",
    "BFGS(F, dF, x_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On normal input and functions method works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F(x):\n",
    "    return np.cos(x) - x ** 3\n",
    "\n",
    "def dF(x):\n",
    "    return -np.sin(x) - 3 * x ** 2\n",
    "\n",
    "x_start = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F(x):\n",
    "    return (x + 5) ** 2\n",
    "\n",
    "def dF(x):\n",
    "    return 2 * (x + 5)\n",
    "\n",
    "x_start = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gradient_descent(x_start, eps=1e-4, alpha = 1e-2):\n",
    "    step_counter = 0\n",
    "    x_cur = x_start\n",
    "    x_new = x_cur - alpha * dF(x_cur)\n",
    "    while np.abs(x_cur - x_new) > eps and np.abs(F(x_new) - F(x_cur)) > eps:\n",
    "        step_counter += 1\n",
    "        if step_counter > 10000:\n",
    "            raise Exception('For eps = %f and alpha = %f method diverges :('%(eps, alpha))\n",
    "        x_cur = x_new\n",
    "        x_new = x_new = x_cur - alpha * dF(x_cur)\n",
    "    print('Optimal solution for eps = %f, alpha = %f founded'%( eps, alpha))\n",
    "    print('steps: %d' %(step_counter))\n",
    "    print('x_optimal =',(x_cur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_newton_method(x_start, eps=1e-10, alpha=1):\n",
    "    step_counter = 0\n",
    "    x_cur = x_start\n",
    "    x_new = x_cur - F(x_cur) / dF(x_cur)\n",
    "    while np.abs(np.abs(x_cur - x_new)) > eps and np.linalg.norm(F(x_new)) > eps:\n",
    "        step_counter += 1\n",
    "        if step_counter > 10000:\n",
    "            raise Exception('For eps = %f and alpha = %f method diverges :('%(eps, alpha))\n",
    "        x_cur = x_new\n",
    "        x_new = x_cur - F(x_cur) / dF(x_cur)\n",
    "    print('Optimal solution for eps = %f, alpha = %f founded'%( eps, alpha))\n",
    "    print('steps: %d' %(step_counter))\n",
    "    print('x_optimal =',(x_cur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal solution for eps = 0.000100, alpha = 0.010000 founded\n",
      "steps: 251\n",
      "x_optimal = -4.949784824391848\n",
      "Optimal solution for eps = 0.000000, alpha = 1.000000 founded\n",
      "steps: 19\n",
      "x_optimal = -4.9999847412109375\n"
     ]
    }
   ],
   "source": [
    "test_gradient_descent(x_start, 1e-4, 1e-2)\n",
    "test_newton_method(x_start, 1e-10, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
